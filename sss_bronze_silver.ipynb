{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81f76c62-7b4a-4318-a7b3-76884df7fc01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.connect.session.SparkSession at 0x7f1596ea9910>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6e83d17-8629-4e06-a86a-2059a744c44e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "sss_schema = StructType([\n",
    "    StructField('row_id', StringType(), True),\n",
    "    StructField('order_id', StringType(), True),\n",
    "    StructField('order_date', StringType(), True),\n",
    "    StructField('ship_date', StringType(), True),\n",
    "    StructField('ship_mode', StringType(), True),\n",
    "    StructField('customer_id', StringType(), True),\n",
    "    StructField('customer_name', StringType(), True),\n",
    "    StructField('segment', StringType(), True),\n",
    "    StructField('country', StringType(), True),\n",
    "    StructField('city', StringType(), True),\n",
    "    StructField('state', StringType(), True),\n",
    "    StructField('postal_code', LongType(), True),\n",
    "    StructField('region', StringType(), True),\n",
    "    StructField('product_id', StringType(), True),\n",
    "    StructField('category', StringType(), True),\n",
    "    StructField('sub_category', StringType(), True),\n",
    "    StructField('product_name', StringType(), True),\n",
    "    StructField('sales', DoubleType(), True),\n",
    "    StructField('quantity', LongType(), True),\n",
    "    StructField('discount', DoubleType(), True),\n",
    "    StructField('profit', DoubleType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92b75c48-1e3b-4933-a319-d31c48e02f21",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "S3_INPUT_PATH = 's3://your-bucket/path/to/input-file'\n",
    "S3_OUTPUT_PATH = 's3://your-bucket/path/to/output-dir/'\n",
    "\n",
    "df = spark.read.format('csv').option('header',True).schema(sss_schema).load(S3_INPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0fb86dc0-d780-42cc-a54e-0d16980dce12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- row_id: string (nullable = true)\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- order_date: string (nullable = true)\n",
      " |-- ship_date: string (nullable = true)\n",
      " |-- ship_mode: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- customer_name: string (nullable = true)\n",
      " |-- segment: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- postal_code: long (nullable = true)\n",
      " |-- region: string (nullable = true)\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- sub_category: string (nullable = true)\n",
      " |-- product_name: string (nullable = true)\n",
      " |-- sales: double (nullable = true)\n",
      " |-- quantity: long (nullable = true)\n",
      " |-- discount: double (nullable = true)\n",
      " |-- profit: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2767e954-7af8-4f1d-90ff-d7f808043e2e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df.drop('row_id')\n",
    "# df.limit(10).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37ac89b2-61f8-483e-a023-e356ccba90f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace\n",
    "\n",
    "df_insert = df.withColumn('order_date', regexp_replace(col('order_date'),r\"^(\\d{1}/)\", r\"0\\1/\"))\\\n",
    "    .withColumn('order_date', regexp_replace(col('order_date'),r\"/(\\d{1})/\", r\"/0\\1/\"))\\\n",
    "    .withColumn('ship_date', regexp_replace(col('ship_date'),r\"^(\\d{1}/)\", r\"0\\1/\"))\\\n",
    "    .withColumn('ship_date', regexp_replace(col('ship_date'),r\"/(\\d{1})/\", r\"/0\\1/\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bba6e5e-77f6-4767-8c4c-219f20d20562",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>order_id</th><th>order_date</th><th>ship_date</th><th>ship_mode</th><th>customer_id</th><th>customer_name</th><th>segment</th><th>country</th><th>city</th><th>state</th><th>postal_code</th><th>region</th><th>product_id</th><th>category</th><th>sub_category</th><th>product_name</th><th>sales</th><th>quantity</th><th>discount</th><th>profit</th></tr></thead><tbody><tr><td>CA-2016-152156</td><td>11/01/2016</td><td>11/11/2016</td><td>Second Class</td><td>CG-12520</td><td>Claire Gute</td><td>Consumer</td><td>United States</td><td>Henderson</td><td>Kentucky</td><td>42420</td><td>South</td><td>FUR-BO-10001798</td><td>Furniture</td><td>Bookcases</td><td>Bush Somerset Collection Bookcase</td><td>261.96</td><td>2</td><td>0.0</td><td>41.9136</td></tr><tr><td>CA-2016-152156</td><td>11/01/2016</td><td>11/11/2016</td><td>Second Class</td><td>CG-12520</td><td>Claire Gute</td><td>Consumer</td><td>United States</td><td>Henderson</td><td>Kentucky</td><td>42420</td><td>South</td><td>FUR-CH-10000454</td><td>Furniture</td><td>Chairs</td><td>Hon Deluxe Fabric Upholstered Stacking Chairs, Rounded Back</td><td>731.94</td><td>3</td><td>0.0</td><td>219.582</td></tr><tr><td>CA-2016-138688</td><td>01/12/2016</td><td>01/16/2016</td><td>Second Class</td><td>DV-13045</td><td>Darrin Van Huff</td><td>Corporate</td><td>United States</td><td>Los Angeles</td><td>California</td><td>90036</td><td>West</td><td>OFF-LA-10000240</td><td>Office Supplies</td><td>Labels</td><td>Self-Adhesive Address Labels for Typewriters by Universal</td><td>14.62</td><td>2</td><td>0.0</td><td>6.8714</td></tr><tr><td>US-2015-108966</td><td>10/11/2015</td><td>10/18/2015</td><td>Standard Class</td><td>SO-20335</td><td>Sean O'Donnell</td><td>Consumer</td><td>United States</td><td>Fort Lauderdale</td><td>Florida</td><td>33311</td><td>South</td><td>FUR-TA-10000577</td><td>Furniture</td><td>Tables</td><td>Bretford CR4500 Series Slim Rectangular Table</td><td>957.5775</td><td>5</td><td>0.45</td><td>-383.031</td></tr><tr><td>US-2015-108966</td><td>10/11/2015</td><td>10/18/2015</td><td>Standard Class</td><td>SO-20335</td><td>Sean O'Donnell</td><td>Consumer</td><td>United States</td><td>Fort Lauderdale</td><td>Florida</td><td>33311</td><td>South</td><td>OFF-ST-10000760</td><td>Office Supplies</td><td>Storage</td><td>Eldon Fold 'N Roll Cart System</td><td>22.368</td><td>2</td><td>0.2</td><td>2.5164</td></tr><tr><td>CA-2014-115812</td><td>01/01/2014</td><td>01/14/2014</td><td>Standard Class</td><td>BH-11710</td><td>Brosina Hoffman</td><td>Consumer</td><td>United States</td><td>Los Angeles</td><td>California</td><td>90032</td><td>West</td><td>FUR-FU-10001487</td><td>Furniture</td><td>Furnishings</td><td>Eldon Expressions Wood and Plastic Desk Accessories, Cherry Wood</td><td>48.86</td><td>7</td><td>0.0</td><td>14.1694</td></tr><tr><td>CA-2014-115812</td><td>01/01/2014</td><td>01/14/2014</td><td>Standard Class</td><td>BH-11710</td><td>Brosina Hoffman</td><td>Consumer</td><td>United States</td><td>Los Angeles</td><td>California</td><td>90032</td><td>West</td><td>OFF-AR-10002833</td><td>Office Supplies</td><td>Art</td><td>Newell 322</td><td>7.28</td><td>4</td><td>0.0</td><td>1.9656</td></tr><tr><td>CA-2014-115812</td><td>01/01/2014</td><td>01/14/2014</td><td>Standard Class</td><td>BH-11710</td><td>Brosina Hoffman</td><td>Consumer</td><td>United States</td><td>Los Angeles</td><td>California</td><td>90032</td><td>West</td><td>TEC-PH-10002275</td><td>Technology</td><td>Phones</td><td>Mitel 5320 IP Phone VoIP phone</td><td>907.152</td><td>6</td><td>0.2</td><td>90.7152</td></tr><tr><td>CA-2014-115812</td><td>01/01/2014</td><td>01/14/2014</td><td>Standard Class</td><td>BH-11710</td><td>Brosina Hoffman</td><td>Consumer</td><td>United States</td><td>Los Angeles</td><td>California</td><td>90032</td><td>West</td><td>OFF-BI-10003910</td><td>Office Supplies</td><td>Binders</td><td>DXL Angle-View Binders with Locking Rings by Samsill</td><td>18.504</td><td>3</td><td>0.2</td><td>5.7825</td></tr><tr><td>CA-2014-115812</td><td>01/01/2014</td><td>01/14/2014</td><td>Standard Class</td><td>BH-11710</td><td>Brosina Hoffman</td><td>Consumer</td><td>United States</td><td>Los Angeles</td><td>California</td><td>90032</td><td>West</td><td>OFF-AP-10002892</td><td>Office Supplies</td><td>Appliances</td><td>Belkin F5C206VTEL 6 Outlet Surge</td><td>114.9</td><td>5</td><td>0.0</td><td>34.47</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "CA-2016-152156",
         "11/01/2016",
         "11/11/2016",
         "Second Class",
         "CG-12520",
         "Claire Gute",
         "Consumer",
         "United States",
         "Henderson",
         "Kentucky",
         42420,
         "South",
         "FUR-BO-10001798",
         "Furniture",
         "Bookcases",
         "Bush Somerset Collection Bookcase",
         261.96,
         2,
         0,
         41.9136
        ],
        [
         "CA-2016-152156",
         "11/01/2016",
         "11/11/2016",
         "Second Class",
         "CG-12520",
         "Claire Gute",
         "Consumer",
         "United States",
         "Henderson",
         "Kentucky",
         42420,
         "South",
         "FUR-CH-10000454",
         "Furniture",
         "Chairs",
         "Hon Deluxe Fabric Upholstered Stacking Chairs, Rounded Back",
         731.94,
         3,
         0,
         219.582
        ],
        [
         "CA-2016-138688",
         "01/12/2016",
         "01/16/2016",
         "Second Class",
         "DV-13045",
         "Darrin Van Huff",
         "Corporate",
         "United States",
         "Los Angeles",
         "California",
         90036,
         "West",
         "OFF-LA-10000240",
         "Office Supplies",
         "Labels",
         "Self-Adhesive Address Labels for Typewriters by Universal",
         14.62,
         2,
         0,
         6.8714
        ],
        [
         "US-2015-108966",
         "10/11/2015",
         "10/18/2015",
         "Standard Class",
         "SO-20335",
         "Sean O'Donnell",
         "Consumer",
         "United States",
         "Fort Lauderdale",
         "Florida",
         33311,
         "South",
         "FUR-TA-10000577",
         "Furniture",
         "Tables",
         "Bretford CR4500 Series Slim Rectangular Table",
         957.5775,
         5,
         0.45,
         -383.031
        ],
        [
         "US-2015-108966",
         "10/11/2015",
         "10/18/2015",
         "Standard Class",
         "SO-20335",
         "Sean O'Donnell",
         "Consumer",
         "United States",
         "Fort Lauderdale",
         "Florida",
         33311,
         "South",
         "OFF-ST-10000760",
         "Office Supplies",
         "Storage",
         "Eldon Fold 'N Roll Cart System",
         22.368,
         2,
         0.2,
         2.5164
        ],
        [
         "CA-2014-115812",
         "01/01/2014",
         "01/14/2014",
         "Standard Class",
         "BH-11710",
         "Brosina Hoffman",
         "Consumer",
         "United States",
         "Los Angeles",
         "California",
         90032,
         "West",
         "FUR-FU-10001487",
         "Furniture",
         "Furnishings",
         "Eldon Expressions Wood and Plastic Desk Accessories, Cherry Wood",
         48.86,
         7,
         0,
         14.1694
        ],
        [
         "CA-2014-115812",
         "01/01/2014",
         "01/14/2014",
         "Standard Class",
         "BH-11710",
         "Brosina Hoffman",
         "Consumer",
         "United States",
         "Los Angeles",
         "California",
         90032,
         "West",
         "OFF-AR-10002833",
         "Office Supplies",
         "Art",
         "Newell 322",
         7.28,
         4,
         0,
         1.9656
        ],
        [
         "CA-2014-115812",
         "01/01/2014",
         "01/14/2014",
         "Standard Class",
         "BH-11710",
         "Brosina Hoffman",
         "Consumer",
         "United States",
         "Los Angeles",
         "California",
         90032,
         "West",
         "TEC-PH-10002275",
         "Technology",
         "Phones",
         "Mitel 5320 IP Phone VoIP phone",
         907.152,
         6,
         0.2,
         90.7152
        ],
        [
         "CA-2014-115812",
         "01/01/2014",
         "01/14/2014",
         "Standard Class",
         "BH-11710",
         "Brosina Hoffman",
         "Consumer",
         "United States",
         "Los Angeles",
         "California",
         90032,
         "West",
         "OFF-BI-10003910",
         "Office Supplies",
         "Binders",
         "DXL Angle-View Binders with Locking Rings by Samsill",
         18.504,
         3,
         0.2,
         5.7825
        ],
        [
         "CA-2014-115812",
         "01/01/2014",
         "01/14/2014",
         "Standard Class",
         "BH-11710",
         "Brosina Hoffman",
         "Consumer",
         "United States",
         "Los Angeles",
         "California",
         90032,
         "West",
         "OFF-AP-10002892",
         "Office Supplies",
         "Appliances",
         "Belkin F5C206VTEL 6 Outlet Surge",
         114.9,
         5,
         0,
         34.47
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "order_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "order_date",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "ship_date",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "ship_mode",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "customer_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "customer_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "segment",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "country",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "city",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "state",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "postal_code",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "region",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "product_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "category",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "sub_category",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "product_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "sales",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "quantity",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "discount",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "profit",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_insert.limit(10).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "245ef4f0-5b3f-4236-86b2-8acf26c3172f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>order_id</th><th>order_date</th><th>ship_date</th><th>ship_mode</th><th>customer_id</th><th>customer_name</th><th>segment</th><th>country</th><th>city</th><th>state</th><th>postal_code</th><th>region</th><th>product_id</th><th>category</th><th>sub_category</th><th>product_name</th><th>sales</th><th>quantity</th><th>discount</th><th>profit</th><th>month(to_date(order_date, MM/dd/yyyy))</th></tr></thead><tbody><tr><td>CA-2016-152156</td><td>11/01/2016</td><td>11/11/2016</td><td>Second Class</td><td>CG-12520</td><td>Claire Gute</td><td>Consumer</td><td>United States</td><td>Henderson</td><td>Kentucky</td><td>42420</td><td>South</td><td>FUR-BO-10001798</td><td>Furniture</td><td>Bookcases</td><td>Bush Somerset Collection Bookcase</td><td>261.96</td><td>2</td><td>0.0</td><td>41.9136</td><td>11</td></tr><tr><td>CA-2016-152156</td><td>11/01/2016</td><td>11/11/2016</td><td>Second Class</td><td>CG-12520</td><td>Claire Gute</td><td>Consumer</td><td>United States</td><td>Henderson</td><td>Kentucky</td><td>42420</td><td>South</td><td>FUR-CH-10000454</td><td>Furniture</td><td>Chairs</td><td>Hon Deluxe Fabric Upholstered Stacking Chairs, Rounded Back</td><td>731.94</td><td>3</td><td>0.0</td><td>219.582</td><td>11</td></tr><tr><td>CA-2016-138688</td><td>01/12/2016</td><td>01/16/2016</td><td>Second Class</td><td>DV-13045</td><td>Darrin Van Huff</td><td>Corporate</td><td>United States</td><td>Los Angeles</td><td>California</td><td>90036</td><td>West</td><td>OFF-LA-10000240</td><td>Office Supplies</td><td>Labels</td><td>Self-Adhesive Address Labels for Typewriters by Universal</td><td>14.62</td><td>2</td><td>0.0</td><td>6.8714</td><td>1</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "CA-2016-152156",
         "11/01/2016",
         "11/11/2016",
         "Second Class",
         "CG-12520",
         "Claire Gute",
         "Consumer",
         "United States",
         "Henderson",
         "Kentucky",
         42420,
         "South",
         "FUR-BO-10001798",
         "Furniture",
         "Bookcases",
         "Bush Somerset Collection Bookcase",
         261.96,
         2,
         0,
         41.9136,
         11
        ],
        [
         "CA-2016-152156",
         "11/01/2016",
         "11/11/2016",
         "Second Class",
         "CG-12520",
         "Claire Gute",
         "Consumer",
         "United States",
         "Henderson",
         "Kentucky",
         42420,
         "South",
         "FUR-CH-10000454",
         "Furniture",
         "Chairs",
         "Hon Deluxe Fabric Upholstered Stacking Chairs, Rounded Back",
         731.94,
         3,
         0,
         219.582,
         11
        ],
        [
         "CA-2016-138688",
         "01/12/2016",
         "01/16/2016",
         "Second Class",
         "DV-13045",
         "Darrin Van Huff",
         "Corporate",
         "United States",
         "Los Angeles",
         "California",
         90036,
         "West",
         "OFF-LA-10000240",
         "Office Supplies",
         "Labels",
         "Self-Adhesive Address Labels for Typewriters by Universal",
         14.62,
         2,
         0,
         6.8714,
         1
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "order_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "order_date",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "ship_date",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "ship_mode",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "customer_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "customer_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "segment",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "country",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "city",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "state",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "postal_code",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "region",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "product_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "category",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "sub_category",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "product_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "sales",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "quantity",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "discount",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "profit",
         "type": "\"double\""
        },
        {
         "metadata": "{\"__autoGeneratedAlias\": \"true\"}",
         "name": "month(to_date(order_date, MM/dd/yyyy))",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_date, date_format, month\n",
    "\n",
    "df_insert.select('*',month(to_date(col('order_date'), format='MM/dd/yyyy'))).limit(3).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9369ce08-729b-4943-8253-eeae2a247cde",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date, date_format\n",
    "\n",
    "df2 = df_insert.withColumn('order_date', to_date(col('order_date'), format='MM/dd/yyyy'))\\\n",
    "                        .withColumn('ship_date', to_date(col('ship_date'), format='MM/dd/yyyy'))\n",
    "    # .limit(10).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78f36102-9235-4db2-8243-1d5b2a852c4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "df2.select(year(col('order_date'))).limit(5).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e9639dd-2643-47cb-8733-2cb982af82a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- order_date: date (nullable = true)\n",
      " |-- ship_date: date (nullable = true)\n",
      " |-- ship_mode: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- customer_name: string (nullable = true)\n",
      " |-- segment: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- postal_code: long (nullable = true)\n",
      " |-- region: string (nullable = true)\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- sub_category: string (nullable = true)\n",
      " |-- product_name: string (nullable = true)\n",
      " |-- sales: double (nullable = true)\n",
      " |-- quantity: long (nullable = true)\n",
      " |-- discount: double (nullable = true)\n",
      " |-- profit: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1265603a-7520-4289-b1f7-55c3bef03687",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>order_date</th></tr></thead><tbody><tr><td>2017-01-15</td></tr><tr><td>2015-11-22</td></tr><tr><td>2015-11-22</td></tr><tr><td>2014-01-13</td></tr><tr><td>2014-01-27</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "2017-01-15"
        ],
        [
         "2015-11-22"
        ],
        [
         "2015-11-22"
        ],
        [
         "2014-01-13"
        ],
        [
         "2014-01-27"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "order_date",
         "type": "\"date\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# check year month day format\n",
    "df2.select('order_date').where(day(col('order_date'))>=13).limit(5).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd6626b0-6a10-46d1-852c-1642b7b6e3fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Lagacy content, don't run. You can run but everything will be the same. So don't waste your compute resource\n",
    "df_date_format = df2.withColumn('order_date', date_format(col('order_date'), 'yyyy-MM-dd'))\\\n",
    "    .withColumn('ship_date', date_format(col('ship_date'), 'yyyy-MM-dd'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b18a86d0-51ec-46b0-a9b6-0cd42a9082b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# make another 2 column for partitioning\n",
    "from pyspark.sql.functions import year, month\n",
    "\n",
    "df_write = df_date_format.withColumn('year', year(col('order_date')))\\\n",
    "                        .withColumn('month', month(col('order_date')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "743edfb9-a841-4e2c-af01-ceede3ac562e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>order_id</th><th>order_date</th><th>ship_date</th><th>ship_mode</th><th>customer_id</th><th>customer_name</th><th>segment</th><th>country</th><th>city</th><th>state</th><th>postal_code</th><th>region</th><th>product_id</th><th>category</th><th>sub_category</th><th>product_name</th><th>sales</th><th>quantity</th><th>discount</th><th>profit</th><th>year</th><th>month</th></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "order_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "order_date",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "ship_date",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "ship_mode",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "customer_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "customer_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "segment",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "country",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "city",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "state",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "postal_code",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "region",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "product_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "category",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "sub_category",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "product_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "sales",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "quantity",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "discount",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "profit",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "year",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "month",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# check for data quality. This is not real-world data so it should be good quality.\n",
    "df_write.select('*').where((col('order_id').isNull()) | (col('customer_id').isNull()) | (col('postal_code').isNull())\\\n",
    "            | (col('product_id').isNull())).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62b99aae-07b0-48b7-9450-b9f4c21e09da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
       "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)\n",
       "File \u001b[0;32m<command-7151363212929843>, line 1\u001b[0m\n",
       "\u001b[0;32m----> 1\u001b[0m spark\u001b[38;5;241m.\u001b[39msql(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCREATE TABLE IF NOT EXISTS rawfile_csv.sss.silver(\u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n",
       "\u001b[1;32m      2\u001b[0m \u001b[38;5;124m    order_id STRING NOT NULL,\u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n",
       "\u001b[1;32m      3\u001b[0m \u001b[38;5;124m    ship_date DATE,\u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n",
       "\u001b[1;32m      4\u001b[0m \u001b[38;5;124m    ship_mode STRING,\u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n",
       "\u001b[1;32m      5\u001b[0m \u001b[38;5;124m    customer_id STRING,\u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n",
       "\u001b[1;32m      6\u001b[0m \u001b[38;5;124m    customer_name STRING,\u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n",
       "\u001b[1;32m      7\u001b[0m \u001b[38;5;124m    segment STRING,\u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n",
       "\u001b[1;32m      8\u001b[0m \u001b[38;5;124m    country STRING,\u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n",
       "\u001b[1;32m      9\u001b[0m \u001b[38;5;124m    city STRING,\u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n",
       "\u001b[1;32m     10\u001b[0m \u001b[38;5;124m    state STRING,\u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n",
       "\u001b[1;32m     11\u001b[0m \u001b[38;5;124m    postal_code LONG,\u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n",
       "\u001b[1;32m     12\u001b[0m \u001b[38;5;124m    region STRING,\u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n",
       "\u001b[1;32m     13\u001b[0m \u001b[38;5;124m    product_id STRING,\u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n",
       "\u001b[1;32m     14\u001b[0m \u001b[38;5;124m    category VARCHAR(200),\u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n",
       "\u001b[1;32m     15\u001b[0m \u001b[38;5;124m    sub_category VARCHAR(200),\u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n",
       "\u001b[1;32m     16\u001b[0m \u001b[38;5;124m    product_name STRING,\u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n",
       "\u001b[1;32m     17\u001b[0m \u001b[38;5;124m    sales DOUBLE,\u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n",
       "\u001b[1;32m     18\u001b[0m \u001b[38;5;124m    quantity LONG,\u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n",
       "\u001b[1;32m     19\u001b[0m \u001b[38;5;124m    discount DOUBLE,\u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n",
       "\u001b[1;32m     20\u001b[0m \u001b[38;5;124m    profit DOUBLE)\u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n",
       "\u001b[1;32m     21\u001b[0m \u001b[38;5;124m    PARTITIONED BY (order_date DATE)\u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n",
       "\u001b[1;32m     22\u001b[0m \u001b[38;5;124m    STORED AS PARQUET\u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n",
       "\u001b[1;32m     23\u001b[0m \u001b[38;5;124m    LOCATION \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Volumes/rawfile_csv/sss/silver\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n",
       "\u001b[1;32m     24\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
       "\n",
       "File \u001b[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/session.py:736\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n",
       "\u001b[1;32m    733\u001b[0m         _views\u001b[38;5;241m.\u001b[39mappend(SubqueryAlias(df\u001b[38;5;241m.\u001b[39m_plan, name))\n",
       "\u001b[1;32m    735\u001b[0m cmd \u001b[38;5;241m=\u001b[39m SQL(sqlQuery, _args, _named_args, _views)\n",
       "\u001b[0;32m--> 736\u001b[0m data, properties, ei \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mexecute_command(cmd\u001b[38;5;241m.\u001b[39mcommand(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client))\n",
       "\u001b[1;32m    737\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msql_command_result\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m properties:\n",
       "\u001b[1;32m    738\u001b[0m     df \u001b[38;5;241m=\u001b[39m DataFrame(CachedRelation(properties[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msql_command_result\u001b[39m\u001b[38;5;124m\"\u001b[39m]), \u001b[38;5;28mself\u001b[39m)\n",
       "\n",
       "File \u001b[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1297\u001b[0m, in \u001b[0;36mSparkConnectClient.execute_command\u001b[0;34m(self, command, observations, extra_request_metadata)\u001b[0m\n",
       "\u001b[1;32m   1295\u001b[0m     req\u001b[38;5;241m.\u001b[39muser_context\u001b[38;5;241m.\u001b[39muser_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_user_id\n",
       "\u001b[1;32m   1296\u001b[0m req\u001b[38;5;241m.\u001b[39mplan\u001b[38;5;241m.\u001b[39mcommand\u001b[38;5;241m.\u001b[39mCopyFrom(command)\n",
       "\u001b[0;32m-> 1297\u001b[0m data, _, metrics, observed_metrics, properties \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_execute_and_fetch(\n",
       "\u001b[1;32m   1298\u001b[0m     req, observations \u001b[38;5;129;01mor\u001b[39;00m {}, extra_request_metadata\n",
       "\u001b[1;32m   1299\u001b[0m )\n",
       "\u001b[1;32m   1300\u001b[0m \u001b[38;5;66;03m# Create a query execution object.\u001b[39;00m\n",
       "\u001b[1;32m   1301\u001b[0m ei \u001b[38;5;241m=\u001b[39m ExecutionInfo(metrics, observed_metrics)\n",
       "\n",
       "File \u001b[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1755\u001b[0m, in \u001b[0;36mSparkConnectClient._execute_and_fetch\u001b[0;34m(self, req, observations, extra_request_metadata, self_destruct)\u001b[0m\n",
       "\u001b[1;32m   1752\u001b[0m properties: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m {}\n",
       "\u001b[1;32m   1754\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Progress(handlers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_progress_handlers, operation_id\u001b[38;5;241m=\u001b[39mreq\u001b[38;5;241m.\u001b[39moperation_id) \u001b[38;5;28;01mas\u001b[39;00m progress:\n",
       "\u001b[0;32m-> 1755\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m response \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_execute_and_fetch_as_iterator(\n",
       "\u001b[1;32m   1756\u001b[0m         req, observations, extra_request_metadata \u001b[38;5;129;01mor\u001b[39;00m [], progress\u001b[38;5;241m=\u001b[39mprogress\n",
       "\u001b[1;32m   1757\u001b[0m     ):\n",
       "\u001b[1;32m   1758\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, StructType):\n",
       "\u001b[1;32m   1759\u001b[0m             schema \u001b[38;5;241m=\u001b[39m response\n",
       "\n",
       "File \u001b[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1731\u001b[0m, in \u001b[0;36mSparkConnectClient._execute_and_fetch_as_iterator\u001b[0;34m(self, req, observations, extra_request_metadata, progress)\u001b[0m\n",
       "\u001b[1;32m   1729\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m kb\n",
       "\u001b[1;32m   1730\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n",
       "\u001b[0;32m-> 1731\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_error(error)\n",
       "\n",
       "File \u001b[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:2047\u001b[0m, in \u001b[0;36mSparkConnectClient._handle_error\u001b[0;34m(self, error)\u001b[0m\n",
       "\u001b[1;32m   2045\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mthread_local\u001b[38;5;241m.\u001b[39minside_error_handling \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
       "\u001b[1;32m   2046\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(error, grpc\u001b[38;5;241m.\u001b[39mRpcError):\n",
       "\u001b[0;32m-> 2047\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_rpc_error(error)\n",
       "\u001b[1;32m   2048\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(error, \u001b[38;5;167;01mValueError\u001b[39;00m):\n",
       "\u001b[1;32m   2049\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot invoke RPC\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(error) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(error):\n",
       "\n",
       "File \u001b[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:2149\u001b[0m, in \u001b[0;36mSparkConnectClient._handle_rpc_error\u001b[0;34m(self, rpc_error)\u001b[0m\n",
       "\u001b[1;32m   2134\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\n",
       "\u001b[1;32m   2135\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPython versions in the Spark Connect client and server are different. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
       "\u001b[1;32m   2136\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo execute user-defined functions, client and server should have the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
       "\u001b[0;32m   (...)\u001b[0m\n",
       "\u001b[1;32m   2145\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://docs.databricks.com/en/release-notes/serverless.html.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
       "\u001b[1;32m   2146\u001b[0m                 )\n",
       "\u001b[1;32m   2147\u001b[0m             \u001b[38;5;66;03m# END-EDGE\u001b[39;00m\n",
       "\u001b[0;32m-> 2149\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m convert_exception(\n",
       "\u001b[1;32m   2150\u001b[0m                 info,\n",
       "\u001b[1;32m   2151\u001b[0m                 status\u001b[38;5;241m.\u001b[39mmessage,\n",
       "\u001b[1;32m   2152\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fetch_enriched_error(info),\n",
       "\u001b[1;32m   2153\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_display_server_stack_trace(),\n",
       "\u001b[1;32m   2154\u001b[0m             ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
       "\u001b[1;32m   2156\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m SparkConnectGrpcException(status\u001b[38;5;241m.\u001b[39mmessage) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
       "\u001b[1;32m   2157\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
       "\n",
       "\u001b[0;31mAnalysisException\u001b[0m: [RequestId=17f6bdfb-1980-4294-a419-d202b9f5425e ErrorClass=INVALID_PARAMETER_VALUE.INVALID_PARAMETER_VALUE] Missing cloud file system scheme\n",
       "\n",
       "JVM stacktrace:\n",
       "com.databricks.sql.managedcatalog.UnityCatalogServiceException\n",
       "\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:126)\n",
       "\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:66)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:266)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7310)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7296)\n",
       "\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.generateTemporaryPathCredentials(ManagedCatalogClientImpl.scala:6277)\n",
       "\tat com.databricks.sql.managedcatalog.ManagedCatalogCommon.generateTemporaryPathCredentials(ManagedCatalogCommon.scala:3236)\n",
       "\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$generateTemporaryPathCredentials$2(ProfiledManagedCatalog.scala:726)\n",
       "\tat org.apache.spark.sql.catalyst.MetricKeyUtils$.measure(MetricKey.scala:1476)\n",
       "\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$profile$1(ProfiledManagedCatalog.scala:64)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.profile(ProfiledManagedCatalog.scala:63)\n",
       "\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.generateTemporaryPathCredentials(ProfiledManagedCatalog.scala:726)\n",
       "\tat com.databricks.unity.CredentialScopeSQLHelper$.checkPathOperations(CredentialScopeSQLHelper.scala:146)\n",
       "\tat com.databricks.unity.CredentialScopeSQLHelper$.register(CredentialScopeSQLHelper.scala:194)\n",
       "\tat com.databricks.unity.CredentialScopeSQLHelper$.registerCreateTableAccess(CredentialScopeSQLHelper.scala:1190)\n",
       "\tat com.databricks.sql.managedcatalog.CredentialScopeTableCredentialHandler.injectCredential(ResolveWithCredential.scala:516)\n",
       "\tat com.databricks.sql.managedcatalog.ResolveWithCredential.com$databricks$sql$managedcatalog$ResolveWithCredential$$maybeDecorateCatalogTable(ResolveWithCredential.scala:83)\n",
       "\tat com.databricks.sql.managedcatalog.ResolveWithCredential$$anonfun$apply$1.applyOrElse(ResolveWithCredential.scala:148)\n",
       "\tat com.databricks.sql.managedcatalog.ResolveWithCredential$$anonfun$apply$1.applyOrElse(ResolveWithCredential.scala:146)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:201)\n",
       "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:201)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:199)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:195)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:42)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown(AnalysisHelper.scala:191)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown$(AnalysisHelper.scala:190)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDown(LogicalPlan.scala:42)\n",
       "\tat com.databricks.sql.managedcatalog.ResolveWithCredential.apply(ResolveWithCredential.scala:146)\n",
       "\tat com.databricks.sql.managedcatalog.ResolveWithCredential.apply(ResolveWithCredential.scala:69)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$17(RuleExecutor.scala:485)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:639)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:623)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:131)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:485)\n",
       "\tat com.databricks.spark.util.MemoryTracker$.withThreadAllocatedBytes(MemoryTracker.scala:51)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.measureRule(QueryPlanningTracker.scala:331)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:483)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:482)\n",
       "\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n",
       "\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n",
       "\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:478)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:452)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23(RuleExecutor.scala:595)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23$adapted(RuleExecutor.scala:595)\n",
       "\tat scala.collection.immutable.List.foreach(List.scala:334)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:595)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:349)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:488)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:481)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:396)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:481)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:415)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:341)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:246)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:341)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:252)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:96)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:131)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:87)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:468)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:468)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:300)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:615)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:690)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:154)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n",
       "\tat com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)\n",
       "\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)\n",
       "\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)\n",
       "\tat com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)\n",
       "\tat com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:61)\n",
       "\tat com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:135)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:690)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1331)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:683)\n",
       "\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:680)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:680)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:294)\n",
       "\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:293)\n",
       "\tat scala.util.Try$.apply(Try.scala:217)\n",
       "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1687)\n",
       "\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n",
       "\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n",
       "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:332)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:273)\n",
       "\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$3(Dataset.scala:154)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n",
       "\tat org.apache.spark.sql.classic.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1157)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.classic.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1157)\n",
       "\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:146)\n",
       "\tat org.apache.spark.sql.classic.SparkSession.$anonfun$sql$4(SparkSession.scala:936)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n",
       "\tat org.apache.spark.sql.classic.SparkSession.sql(SparkSession.scala:888)\n",
       "\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.executeSQL(SparkConnectPlanner.scala:3382)\n",
       "\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleSqlCommand(SparkConnectPlanner.scala:3211)\n",
       "\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3089)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:419)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:316)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:237)\n",
       "\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1748)\n",
       "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:631)\n",
       "\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.$anonfun$onSqlStart$14(SqlGatewayHistorySparkListener.scala:678)\n",
       "\tat scala.Option.map(Option.scala:242)\n",
       "\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.$anonfun$onSqlStart$13(SqlGatewayHistorySparkListener.scala:678)\n",
       "\tat scala.Option.foreach(Option.scala:437)\n",
       "\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.$anonfun$onSqlStart$1(SqlGatewayHistorySparkListener.scala:675)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.com$databricks$spark$sqlgateway$history$SqlGatewayHistorySparkListener$$onSqlStart(SqlGatewayHistorySparkListener.scala:617)\n",
       "\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener$$anonfun$onOtherEventDefault$1.applyOrElse(SqlGatewayHistorySparkListener.scala:230)\n",
       "\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener$$anonfun$onOtherEventDefault$1.applyOrElse(SqlGatewayHistorySparkListener.scala:218)\n",
       "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35)\n",
       "\tat com.databricks.spark.sqlgateway.history.utils.RuntimeVersionBridge$$anonfun$onOtherEvent$1.applyOrElse(RuntimeVersionBridge.scala:209)\n",
       "\tat com.databricks.spark.sqlgateway.history.utils.RuntimeVersionBridge$$anonfun$onOtherEvent$1.applyOrElse(RuntimeVersionBridge.scala:209)\n",
       "\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.$anonfun$onOtherEvent$1(SqlGatewayHistorySparkListener.scala:196)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.onOtherEvent(SqlGatewayHistorySparkListener.scala:196)\n",
       "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:108)\n",
       "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
       "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:42)\n",
       "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:42)\n",
       "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:199)\n",
       "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:169)\n",
       "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:116)\n",
       "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:116)\n",
       "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.scala:17)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n",
       "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:111)\n",
       "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:107)\n",
       "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1575)\n",
       "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:107)"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "AnalysisException",
        "evalue": "[RequestId=17f6bdfb-1980-4294-a419-d202b9f5425e ErrorClass=INVALID_PARAMETER_VALUE.INVALID_PARAMETER_VALUE] Missing cloud file system scheme\n\nJVM stacktrace:\ncom.databricks.sql.managedcatalog.UnityCatalogServiceException\n\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:126)\n\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:66)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:266)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7310)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7296)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.generateTemporaryPathCredentials(ManagedCatalogClientImpl.scala:6277)\n\tat com.databricks.sql.managedcatalog.ManagedCatalogCommon.generateTemporaryPathCredentials(ManagedCatalogCommon.scala:3236)\n\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$generateTemporaryPathCredentials$2(ProfiledManagedCatalog.scala:726)\n\tat org.apache.spark.sql.catalyst.MetricKeyUtils$.measure(MetricKey.scala:1476)\n\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$profile$1(ProfiledManagedCatalog.scala:64)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.profile(ProfiledManagedCatalog.scala:63)\n\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.generateTemporaryPathCredentials(ProfiledManagedCatalog.scala:726)\n\tat com.databricks.unity.CredentialScopeSQLHelper$.checkPathOperations(CredentialScopeSQLHelper.scala:146)\n\tat com.databricks.unity.CredentialScopeSQLHelper$.register(CredentialScopeSQLHelper.scala:194)\n\tat com.databricks.unity.CredentialScopeSQLHelper$.registerCreateTableAccess(CredentialScopeSQLHelper.scala:1190)\n\tat com.databricks.sql.managedcatalog.CredentialScopeTableCredentialHandler.injectCredential(ResolveWithCredential.scala:516)\n\tat com.databricks.sql.managedcatalog.ResolveWithCredential.com$databricks$sql$managedcatalog$ResolveWithCredential$$maybeDecorateCatalogTable(ResolveWithCredential.scala:83)\n\tat com.databricks.sql.managedcatalog.ResolveWithCredential$$anonfun$apply$1.applyOrElse(ResolveWithCredential.scala:148)\n\tat com.databricks.sql.managedcatalog.ResolveWithCredential$$anonfun$apply$1.applyOrElse(ResolveWithCredential.scala:146)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:199)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:195)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown(AnalysisHelper.scala:191)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown$(AnalysisHelper.scala:190)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDown(LogicalPlan.scala:42)\n\tat com.databricks.sql.managedcatalog.ResolveWithCredential.apply(ResolveWithCredential.scala:146)\n\tat com.databricks.sql.managedcatalog.ResolveWithCredential.apply(ResolveWithCredential.scala:69)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$17(RuleExecutor.scala:485)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:639)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:623)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:131)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:485)\n\tat com.databricks.spark.util.MemoryTracker$.withThreadAllocatedBytes(MemoryTracker.scala:51)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.measureRule(QueryPlanningTracker.scala:331)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:483)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:482)\n\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:478)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:452)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23(RuleExecutor.scala:595)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23$adapted(RuleExecutor.scala:595)\n\tat scala.collection.immutable.List.foreach(List.scala:334)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:595)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:349)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:488)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:481)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:396)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:481)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:415)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:341)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:246)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:341)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:252)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:96)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:131)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:87)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:468)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:468)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:300)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:615)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:690)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:154)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n\tat com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)\n\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)\n\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)\n\tat com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)\n\tat com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:61)\n\tat com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)\n\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:135)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:690)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1331)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:683)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:680)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:680)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:294)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:293)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1687)\n\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:332)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:273)\n\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$3(Dataset.scala:154)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.classic.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1157)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.classic.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1157)\n\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:146)\n\tat org.apache.spark.sql.classic.SparkSession.$anonfun$sql$4(SparkSession.scala:936)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.classic.SparkSession.sql(SparkSession.scala:888)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.executeSQL(SparkConnectPlanner.scala:3382)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleSqlCommand(SparkConnectPlanner.scala:3211)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3089)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:419)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:316)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:237)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1748)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:631)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.$anonfun$onSqlStart$14(SqlGatewayHistorySparkListener.scala:678)\n\tat scala.Option.map(Option.scala:242)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.$anonfun$onSqlStart$13(SqlGatewayHistorySparkListener.scala:678)\n\tat scala.Option.foreach(Option.scala:437)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.$anonfun$onSqlStart$1(SqlGatewayHistorySparkListener.scala:675)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.com$databricks$spark$sqlgateway$history$SqlGatewayHistorySparkListener$$onSqlStart(SqlGatewayHistorySparkListener.scala:617)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener$$anonfun$onOtherEventDefault$1.applyOrElse(SqlGatewayHistorySparkListener.scala:230)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener$$anonfun$onOtherEventDefault$1.applyOrElse(SqlGatewayHistorySparkListener.scala:218)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35)\n\tat com.databricks.spark.sqlgateway.history.utils.RuntimeVersionBridge$$anonfun$onOtherEvent$1.applyOrElse(RuntimeVersionBridge.scala:209)\n\tat com.databricks.spark.sqlgateway.history.utils.RuntimeVersionBridge$$anonfun$onOtherEvent$1.applyOrElse(RuntimeVersionBridge.scala:209)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.$anonfun$onOtherEvent$1(SqlGatewayHistorySparkListener.scala:196)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.onOtherEvent(SqlGatewayHistorySparkListener.scala:196)\n\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:108)\n\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:42)\n\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:42)\n\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:199)\n\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:169)\n\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:116)\n\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:116)\n\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.scala:17)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:111)\n\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:107)\n\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1575)\n\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:107)"
       },
       "metadata": {
        "errorSummary": "[RequestId=17f6bdfb-1980-4294-a419-d202b9f5425e ErrorClass=INVALID_PARAMETER_VALUE.INVALID_PARAMETER_VALUE] Missing cloud file system scheme\n\nJVM stacktrace:\ncom.databricks.sql.managedcatalog.UnityCatalogServiceException\n\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:126)\n\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:66)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:266)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7310)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7296)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.generateTemporaryPathCredentials(ManagedCatalogClientImpl.scala:6277)\n\tat com.databricks.sql.managedcatalog.ManagedCatalogCommon.generateTemporaryPathCredentials(ManagedCatalogCommon.scala:3236)\n\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$generateTemporaryPathCredentials$2(ProfiledManagedCatalog.scala:726)\n\tat org.apache.spark.sql.catalyst.MetricKeyUtils$.measure(MetricKey.scala:1476)\n\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$profile$1(ProfiledManagedCatalog.scala:64)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.profile(ProfiledManagedCatalog.scala:63)\n\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.generateTemporaryPathCredentials(ProfiledManagedCatalog.scala:726)\n\tat com.databricks.unity.CredentialScopeSQLHelper$.checkPathOperations(CredentialScopeSQLHelper.scala:146)\n\tat com.databricks.unity.CredentialScopeSQLHelper$.register(CredentialScopeSQLHelper.scala:194)\n\tat com.databricks.unity.CredentialScopeSQLHelper$.registerCreateTableAccess(CredentialScopeSQLHelper.scala:1190)\n\tat com.databricks.sql.managedcatalog.CredentialScopeTableCredentialHandler.injectCredential(ResolveWithCredential.scala:516)\n\tat com.databricks.sql.managedcatalog.ResolveWithCredential.com$databricks$sql$managedcatalog$ResolveWithCredential$$maybeDecorateCatalogTable(ResolveWithCredential.scala:83)\n\tat com.databricks.sql.managedcatalog.ResolveWithCredential$$anonfun$apply$1.applyOrElse(ResolveWithCredential.scala:148)\n\tat com.databricks.sql.managedcatalog.ResolveWithCredential$$anonfun$apply$1.applyOrElse(ResolveWithCredential.scala:146)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:199)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:195)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown(AnalysisHelper.scala:191)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown$(AnalysisHelper.scala:190)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDown(LogicalPlan.scala:42)\n\tat com.databricks.sql.managedcatalog.ResolveWithCredential.apply(ResolveWithCredential.scala:146)\n\tat com.databricks.sql.managedcatalog.ResolveWithCredential.apply(ResolveWithCredential.scala:69)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$17(RuleExecutor.scala:485)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:639)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:623)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:131)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:485)\n\tat com.databricks.spark.util.MemoryTracker$.withThreadAllocatedBytes(MemoryTracker.scala:51)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.measureRule(QueryPlanningTracker.scala:331)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:483)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:482)\n\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:478)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:452)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23(RuleExecutor.scala:595)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23$adapted(RuleExecutor.scala:595)\n\tat scala.collection.immutable.List.foreach(List.scala:334)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:595)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:349)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:488)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:481)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:396)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:481)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:415)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:341)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:246)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:341)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:252)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:96)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:131)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:87)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:468)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:468)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:300)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:615)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:690)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:154)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n\tat com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)\n\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)\n\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)\n\tat com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)\n\tat com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:61)\n\tat com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)\n\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:135)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:690)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1331)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:683)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:680)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:680)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:294)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:293)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1687)\n\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:332)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:273)\n\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$3(Dataset.scala:154)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.classic.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1157)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.classic.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1157)\n\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:146)\n\tat org.apache.spark.sql.classic.SparkSession.$anonfun$sql$4(SparkSession.scala:936)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.classic.SparkSession.sql(SparkSession.scala:888)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.executeSQL(SparkConnectPlanner.scala:3382)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleSqlCommand(SparkConnectPlanner.scala:3211)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3089)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:419)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:316)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:237)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1748)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:631)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.$anonfun$onSqlStart$14(SqlGatewayHistorySparkListener.scala:678)\n\tat scala.Option.map(Option.scala:242)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.$anonfun$onSqlStart$13(SqlGatewayHistorySparkListener.scala:678)\n\tat scala.Option.foreach(Option.scala:437)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.$anonfun$onSqlStart$1(SqlGatewayHistorySparkListener.scala:675)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.com$databricks$spark$sqlgateway$history$SqlGatewayHistorySparkListener$$onSqlStart(SqlGatewayHistorySparkListener.scala:617)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener$$anonfun$onOtherEventDefault$1.applyOrElse(SqlGatewayHistorySparkListener.scala:230)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener$$anonfun$onOtherEventDefault$1.applyOrElse(SqlGatewayHistorySparkListener.scala:218)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35)\n\tat com.databricks.spark.sqlgateway.history.utils.RuntimeVersionBridge$$anonfun$onOtherEvent$1.applyOrElse(RuntimeVersionBridge.scala:209)\n\tat com.databricks.spark.sqlgateway.history.utils.RuntimeVersionBridge$$anonfun$onOtherEvent$1.applyOrElse(RuntimeVersionBridge.scala:209)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.$anonfun$onOtherEvent$1(SqlGatewayHistorySparkListener.scala:196)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.onOtherEvent(SqlGatewayHistorySparkListener.scala:196)\n\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:108)\n\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:42)\n\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:42)\n\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:199)\n\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:169)\n\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:116)\n\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:116)\n\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.scala:17)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:111)\n\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:107)\n\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1575)\n\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:107)"
       },
       "removedWidgets": [],
       "sqlProps": {
        "errorClass": "",
        "pysparkCallSite": "",
        "pysparkFragment": "",
        "pysparkSummary": null,
        "sqlState": null,
        "stackTrace": "com.databricks.sql.managedcatalog.UnityCatalogServiceException\n\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:126)\n\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:66)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:266)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7310)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7296)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.generateTemporaryPathCredentials(ManagedCatalogClientImpl.scala:6277)\n\tat com.databricks.sql.managedcatalog.ManagedCatalogCommon.generateTemporaryPathCredentials(ManagedCatalogCommon.scala:3236)\n\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$generateTemporaryPathCredentials$2(ProfiledManagedCatalog.scala:726)\n\tat org.apache.spark.sql.catalyst.MetricKeyUtils$.measure(MetricKey.scala:1476)\n\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$profile$1(ProfiledManagedCatalog.scala:64)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.profile(ProfiledManagedCatalog.scala:63)\n\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.generateTemporaryPathCredentials(ProfiledManagedCatalog.scala:726)\n\tat com.databricks.unity.CredentialScopeSQLHelper$.checkPathOperations(CredentialScopeSQLHelper.scala:146)\n\tat com.databricks.unity.CredentialScopeSQLHelper$.register(CredentialScopeSQLHelper.scala:194)\n\tat com.databricks.unity.CredentialScopeSQLHelper$.registerCreateTableAccess(CredentialScopeSQLHelper.scala:1190)\n\tat com.databricks.sql.managedcatalog.CredentialScopeTableCredentialHandler.injectCredential(ResolveWithCredential.scala:516)\n\tat com.databricks.sql.managedcatalog.ResolveWithCredential.com$databricks$sql$managedcatalog$ResolveWithCredential$$maybeDecorateCatalogTable(ResolveWithCredential.scala:83)\n\tat com.databricks.sql.managedcatalog.ResolveWithCredential$$anonfun$apply$1.applyOrElse(ResolveWithCredential.scala:148)\n\tat com.databricks.sql.managedcatalog.ResolveWithCredential$$anonfun$apply$1.applyOrElse(ResolveWithCredential.scala:146)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:199)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:195)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown(AnalysisHelper.scala:191)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown$(AnalysisHelper.scala:190)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDown(LogicalPlan.scala:42)\n\tat com.databricks.sql.managedcatalog.ResolveWithCredential.apply(ResolveWithCredential.scala:146)\n\tat com.databricks.sql.managedcatalog.ResolveWithCredential.apply(ResolveWithCredential.scala:69)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$17(RuleExecutor.scala:485)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:639)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:623)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:131)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:485)\n\tat com.databricks.spark.util.MemoryTracker$.withThreadAllocatedBytes(MemoryTracker.scala:51)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.measureRule(QueryPlanningTracker.scala:331)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:483)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:482)\n\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:478)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:452)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23(RuleExecutor.scala:595)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23$adapted(RuleExecutor.scala:595)\n\tat scala.collection.immutable.List.foreach(List.scala:334)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:595)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:349)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:488)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:481)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:396)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:481)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:415)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:341)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:246)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:341)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:252)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:96)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:131)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:87)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:468)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:468)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:300)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:615)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:690)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:154)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n\tat com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)\n\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)\n\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)\n\tat com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)\n\tat com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:61)\n\tat com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)\n\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:135)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:690)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1331)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:683)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:680)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:680)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:294)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:293)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1687)\n\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:332)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:273)\n\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$3(Dataset.scala:154)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.classic.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1157)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.classic.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1157)\n\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:146)\n\tat org.apache.spark.sql.classic.SparkSession.$anonfun$sql$4(SparkSession.scala:936)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.classic.SparkSession.sql(SparkSession.scala:888)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.executeSQL(SparkConnectPlanner.scala:3382)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleSqlCommand(SparkConnectPlanner.scala:3211)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3089)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:419)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:316)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:237)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1748)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:631)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.$anonfun$onSqlStart$14(SqlGatewayHistorySparkListener.scala:678)\n\tat scala.Option.map(Option.scala:242)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.$anonfun$onSqlStart$13(SqlGatewayHistorySparkListener.scala:678)\n\tat scala.Option.foreach(Option.scala:437)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.$anonfun$onSqlStart$1(SqlGatewayHistorySparkListener.scala:675)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.com$databricks$spark$sqlgateway$history$SqlGatewayHistorySparkListener$$onSqlStart(SqlGatewayHistorySparkListener.scala:617)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener$$anonfun$onOtherEventDefault$1.applyOrElse(SqlGatewayHistorySparkListener.scala:230)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener$$anonfun$onOtherEventDefault$1.applyOrElse(SqlGatewayHistorySparkListener.scala:218)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35)\n\tat com.databricks.spark.sqlgateway.history.utils.RuntimeVersionBridge$$anonfun$onOtherEvent$1.applyOrElse(RuntimeVersionBridge.scala:209)\n\tat com.databricks.spark.sqlgateway.history.utils.RuntimeVersionBridge$$anonfun$onOtherEvent$1.applyOrElse(RuntimeVersionBridge.scala:209)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.$anonfun$onOtherEvent$1(SqlGatewayHistorySparkListener.scala:196)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.onOtherEvent(SqlGatewayHistorySparkListener.scala:196)\n\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:108)\n\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:42)\n\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:42)\n\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:199)\n\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:169)\n\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:116)\n\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:116)\n\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.scala:17)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:111)\n\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:107)\n\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1575)\n\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:107)",
        "startIndex": null,
        "stopIndex": null
       },
       "stackFrames": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
        "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
        "File \u001b[0;32m<command-7151363212929843>, line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m spark\u001b[38;5;241m.\u001b[39msql(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCREATE TABLE IF NOT EXISTS rawfile_csv.sss.silver(\u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124m    order_id STRING NOT NULL,\u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124m    ship_date DATE,\u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124m    ship_mode STRING,\u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124m    customer_id STRING,\u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124m    customer_name STRING,\u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124m    segment STRING,\u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124m    country STRING,\u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124m    city STRING,\u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124m    state STRING,\u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124m    postal_code LONG,\u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124m    region STRING,\u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124m    product_id STRING,\u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124m    category VARCHAR(200),\u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124m    sub_category VARCHAR(200),\u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124m    product_name STRING,\u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124m    sales DOUBLE,\u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124m    quantity LONG,\u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124m    discount DOUBLE,\u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;124m    profit DOUBLE)\u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;124m    PARTITIONED BY (order_date DATE)\u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;124m    STORED AS PARQUET\u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;124m    LOCATION \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Volumes/rawfile_csv/sss/silver\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
        "File \u001b[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/session.py:736\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m    733\u001b[0m         _views\u001b[38;5;241m.\u001b[39mappend(SubqueryAlias(df\u001b[38;5;241m.\u001b[39m_plan, name))\n\u001b[1;32m    735\u001b[0m cmd \u001b[38;5;241m=\u001b[39m SQL(sqlQuery, _args, _named_args, _views)\n\u001b[0;32m--> 736\u001b[0m data, properties, ei \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mexecute_command(cmd\u001b[38;5;241m.\u001b[39mcommand(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client))\n\u001b[1;32m    737\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msql_command_result\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m properties:\n\u001b[1;32m    738\u001b[0m     df \u001b[38;5;241m=\u001b[39m DataFrame(CachedRelation(properties[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msql_command_result\u001b[39m\u001b[38;5;124m\"\u001b[39m]), \u001b[38;5;28mself\u001b[39m)\n",
        "File \u001b[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1297\u001b[0m, in \u001b[0;36mSparkConnectClient.execute_command\u001b[0;34m(self, command, observations, extra_request_metadata)\u001b[0m\n\u001b[1;32m   1295\u001b[0m     req\u001b[38;5;241m.\u001b[39muser_context\u001b[38;5;241m.\u001b[39muser_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_user_id\n\u001b[1;32m   1296\u001b[0m req\u001b[38;5;241m.\u001b[39mplan\u001b[38;5;241m.\u001b[39mcommand\u001b[38;5;241m.\u001b[39mCopyFrom(command)\n\u001b[0;32m-> 1297\u001b[0m data, _, metrics, observed_metrics, properties \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_execute_and_fetch(\n\u001b[1;32m   1298\u001b[0m     req, observations \u001b[38;5;129;01mor\u001b[39;00m {}, extra_request_metadata\n\u001b[1;32m   1299\u001b[0m )\n\u001b[1;32m   1300\u001b[0m \u001b[38;5;66;03m# Create a query execution object.\u001b[39;00m\n\u001b[1;32m   1301\u001b[0m ei \u001b[38;5;241m=\u001b[39m ExecutionInfo(metrics, observed_metrics)\n",
        "File \u001b[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1755\u001b[0m, in \u001b[0;36mSparkConnectClient._execute_and_fetch\u001b[0;34m(self, req, observations, extra_request_metadata, self_destruct)\u001b[0m\n\u001b[1;32m   1752\u001b[0m properties: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m   1754\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Progress(handlers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_progress_handlers, operation_id\u001b[38;5;241m=\u001b[39mreq\u001b[38;5;241m.\u001b[39moperation_id) \u001b[38;5;28;01mas\u001b[39;00m progress:\n\u001b[0;32m-> 1755\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m response \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_execute_and_fetch_as_iterator(\n\u001b[1;32m   1756\u001b[0m         req, observations, extra_request_metadata \u001b[38;5;129;01mor\u001b[39;00m [], progress\u001b[38;5;241m=\u001b[39mprogress\n\u001b[1;32m   1757\u001b[0m     ):\n\u001b[1;32m   1758\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, StructType):\n\u001b[1;32m   1759\u001b[0m             schema \u001b[38;5;241m=\u001b[39m response\n",
        "File \u001b[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1731\u001b[0m, in \u001b[0;36mSparkConnectClient._execute_and_fetch_as_iterator\u001b[0;34m(self, req, observations, extra_request_metadata, progress)\u001b[0m\n\u001b[1;32m   1729\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m kb\n\u001b[1;32m   1730\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[0;32m-> 1731\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_error(error)\n",
        "File \u001b[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:2047\u001b[0m, in \u001b[0;36mSparkConnectClient._handle_error\u001b[0;34m(self, error)\u001b[0m\n\u001b[1;32m   2045\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mthread_local\u001b[38;5;241m.\u001b[39minside_error_handling \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   2046\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(error, grpc\u001b[38;5;241m.\u001b[39mRpcError):\n\u001b[0;32m-> 2047\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_rpc_error(error)\n\u001b[1;32m   2048\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(error, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[1;32m   2049\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot invoke RPC\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(error) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(error):\n",
        "File \u001b[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:2149\u001b[0m, in \u001b[0;36mSparkConnectClient._handle_rpc_error\u001b[0;34m(self, rpc_error)\u001b[0m\n\u001b[1;32m   2134\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\n\u001b[1;32m   2135\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPython versions in the Spark Connect client and server are different. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2136\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo execute user-defined functions, client and server should have the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2145\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://docs.databricks.com/en/release-notes/serverless.html.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2146\u001b[0m                 )\n\u001b[1;32m   2147\u001b[0m             \u001b[38;5;66;03m# END-EDGE\u001b[39;00m\n\u001b[0;32m-> 2149\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m convert_exception(\n\u001b[1;32m   2150\u001b[0m                 info,\n\u001b[1;32m   2151\u001b[0m                 status\u001b[38;5;241m.\u001b[39mmessage,\n\u001b[1;32m   2152\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fetch_enriched_error(info),\n\u001b[1;32m   2153\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_display_server_stack_trace(),\n\u001b[1;32m   2154\u001b[0m             ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2156\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m SparkConnectGrpcException(status\u001b[38;5;241m.\u001b[39mmessage) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2157\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
        "\u001b[0;31mAnalysisException\u001b[0m: [RequestId=17f6bdfb-1980-4294-a419-d202b9f5425e ErrorClass=INVALID_PARAMETER_VALUE.INVALID_PARAMETER_VALUE] Missing cloud file system scheme\n\nJVM stacktrace:\ncom.databricks.sql.managedcatalog.UnityCatalogServiceException\n\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException(ErrorDetailsHandler.scala:126)\n\tat com.databricks.managedcatalog.ErrorDetailsHandler.wrapServiceException$(ErrorDetailsHandler.scala:66)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.wrapServiceException(ManagedCatalogClientImpl.scala:266)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapExceptionBase(ManagedCatalogClientImpl.scala:7310)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.recordAndWrapException(ManagedCatalogClientImpl.scala:7296)\n\tat com.databricks.managedcatalog.ManagedCatalogClientImpl.generateTemporaryPathCredentials(ManagedCatalogClientImpl.scala:6277)\n\tat com.databricks.sql.managedcatalog.ManagedCatalogCommon.generateTemporaryPathCredentials(ManagedCatalogCommon.scala:3236)\n\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$generateTemporaryPathCredentials$2(ProfiledManagedCatalog.scala:726)\n\tat org.apache.spark.sql.catalyst.MetricKeyUtils$.measure(MetricKey.scala:1476)\n\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.$anonfun$profile$1(ProfiledManagedCatalog.scala:64)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.profile(ProfiledManagedCatalog.scala:63)\n\tat com.databricks.sql.managedcatalog.ProfiledManagedCatalog.generateTemporaryPathCredentials(ProfiledManagedCatalog.scala:726)\n\tat com.databricks.unity.CredentialScopeSQLHelper$.checkPathOperations(CredentialScopeSQLHelper.scala:146)\n\tat com.databricks.unity.CredentialScopeSQLHelper$.register(CredentialScopeSQLHelper.scala:194)\n\tat com.databricks.unity.CredentialScopeSQLHelper$.registerCreateTableAccess(CredentialScopeSQLHelper.scala:1190)\n\tat com.databricks.sql.managedcatalog.CredentialScopeTableCredentialHandler.injectCredential(ResolveWithCredential.scala:516)\n\tat com.databricks.sql.managedcatalog.ResolveWithCredential.com$databricks$sql$managedcatalog$ResolveWithCredential$$maybeDecorateCatalogTable(ResolveWithCredential.scala:83)\n\tat com.databricks.sql.managedcatalog.ResolveWithCredential$$anonfun$apply$1.applyOrElse(ResolveWithCredential.scala:148)\n\tat com.databricks.sql.managedcatalog.ResolveWithCredential$$anonfun$apply$1.applyOrElse(ResolveWithCredential.scala:146)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:199)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:195)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown(AnalysisHelper.scala:191)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown$(AnalysisHelper.scala:190)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDown(LogicalPlan.scala:42)\n\tat com.databricks.sql.managedcatalog.ResolveWithCredential.apply(ResolveWithCredential.scala:146)\n\tat com.databricks.sql.managedcatalog.ResolveWithCredential.apply(ResolveWithCredential.scala:69)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$17(RuleExecutor.scala:485)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:639)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:623)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:131)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:485)\n\tat com.databricks.spark.util.MemoryTracker$.withThreadAllocatedBytes(MemoryTracker.scala:51)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.measureRule(QueryPlanningTracker.scala:331)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:483)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:482)\n\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:478)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:452)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23(RuleExecutor.scala:595)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23$adapted(RuleExecutor.scala:595)\n\tat scala.collection.immutable.List.foreach(List.scala:334)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:595)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:349)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:488)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:481)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:396)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:481)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:415)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:341)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:246)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:341)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:252)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:96)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:131)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:87)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:468)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:468)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:300)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:615)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:690)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:154)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n\tat com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)\n\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)\n\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)\n\tat com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)\n\tat com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:61)\n\tat com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)\n\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:135)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:690)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1331)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:683)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:680)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:680)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:294)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:293)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1687)\n\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:332)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:273)\n\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$3(Dataset.scala:154)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.classic.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1157)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.classic.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1157)\n\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:146)\n\tat org.apache.spark.sql.classic.SparkSession.$anonfun$sql$4(SparkSession.scala:936)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.classic.SparkSession.sql(SparkSession.scala:888)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.executeSQL(SparkConnectPlanner.scala:3382)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleSqlCommand(SparkConnectPlanner.scala:3211)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3089)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:419)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:316)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:237)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1748)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:631)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.$anonfun$onSqlStart$14(SqlGatewayHistorySparkListener.scala:678)\n\tat scala.Option.map(Option.scala:242)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.$anonfun$onSqlStart$13(SqlGatewayHistorySparkListener.scala:678)\n\tat scala.Option.foreach(Option.scala:437)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.$anonfun$onSqlStart$1(SqlGatewayHistorySparkListener.scala:675)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.com$databricks$spark$sqlgateway$history$SqlGatewayHistorySparkListener$$onSqlStart(SqlGatewayHistorySparkListener.scala:617)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener$$anonfun$onOtherEventDefault$1.applyOrElse(SqlGatewayHistorySparkListener.scala:230)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener$$anonfun$onOtherEventDefault$1.applyOrElse(SqlGatewayHistorySparkListener.scala:218)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35)\n\tat com.databricks.spark.sqlgateway.history.utils.RuntimeVersionBridge$$anonfun$onOtherEvent$1.applyOrElse(RuntimeVersionBridge.scala:209)\n\tat com.databricks.spark.sqlgateway.history.utils.RuntimeVersionBridge$$anonfun$onOtherEvent$1.applyOrElse(RuntimeVersionBridge.scala:209)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.$anonfun$onOtherEvent$1(SqlGatewayHistorySparkListener.scala:196)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.onOtherEvent(SqlGatewayHistorySparkListener.scala:196)\n\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:108)\n\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:42)\n\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:42)\n\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:199)\n\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:169)\n\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:116)\n\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:116)\n\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.scala:17)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:111)\n\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:107)\n\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1575)\n\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:107)"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# You should setup your AWS and Databricks before run this command\n",
    "spark.sql(f\"CREATE TABLE IF NOT EXISTS rawfile_csv.sss.silver(\\\n",
    "    order_id STRING NOT NULL,\\\n",
    "    ship_date DATE,\\\n",
    "    ship_mode STRING,\\\n",
    "    customer_id STRING,\\\n",
    "    customer_name STRING,\\\n",
    "    segment STRING,\\\n",
    "    country STRING,\\\n",
    "    city STRING,\\\n",
    "    state STRING,\\\n",
    "    postal_code LONG,\\\n",
    "    region STRING,\\\n",
    "    product_id STRING,\\\n",
    "    category VARCHAR(200),\\\n",
    "    sub_category VARCHAR(200),\\\n",
    "    product_name STRING,\\\n",
    "    sales DOUBLE,\\\n",
    "    quantity LONG,\\\n",
    "    discount DOUBLE,\\\n",
    "    profit DOUBLE)\\\n",
    "    PARTITIONED BY (order_date DATE)\\\n",
    "    STORED AS PARQUET\\\n",
    "    LOCATION '{S3_OUTPUT_PATH}'\\\n",
    "    \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1b61e597-4f15-4425-8b9b-8cd08ababdb5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_write.write.partitionBy(col('year'), col('month')).insertInto('rawfile_csv.sss.silver')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4e948d07-d978-43fc-87cf-5cbef49b0c28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### The commands below are for write-only, used for practice\n",
    "In the real world scenario, we typically create table and insert like the above. Then, our data will be queryable. \n",
    "  \n",
    "But if you have a problem with your **Databricks on AWS** and still want to learn about Medallion, I suggest you just use the command below then read the parquet and make it a view to make it queryable. Keep in my that, this is not a good practice, just for practice for understanding the concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56f754a8-5e9a-45a2-84eb-dccc6e6c49fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('create volume if not exists path.to.volume')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c2b9efd-46cc-4d07-8ad4-27320b1740d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_write.write.parquet('/Volumes/rawfile_csv/sss/silver/', mode='overwrite', partitionBy=['year', 'month'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5eeef2fe-6b27-4e01-ab30-50ff467a7915",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>order_id</th><th>order_date</th><th>ship_date</th><th>ship_mode</th><th>customer_id</th><th>customer_name</th><th>segment</th><th>country</th><th>city</th><th>state</th><th>postal_code</th><th>region</th><th>product_id</th><th>category</th><th>sub_category</th><th>product_name</th><th>sales</th><th>quantity</th><th>discount</th><th>profit</th><th>year</th><th>month</th></tr></thead><tbody><tr><td>US-2014-150126</td><td>2014-01-27</td><td>2014-01-01</td><td>Standard Class</td><td>AS-10045</td><td>Aaron Smayling</td><td>Corporate</td><td>United States</td><td>New York City</td><td>New York</td><td>10035</td><td>East</td><td>OFF-PA-10002709</td><td>Office Supplies</td><td>Paper</td><td>Xerox 1956</td><td>65.78</td><td>11</td><td>0.0</td><td>32.2322</td><td>2014</td><td>1</td></tr><tr><td>US-2014-121734</td><td>2014-01-11</td><td>2014-01-16</td><td>Standard Class</td><td>SE-20110</td><td>Sanjit Engle</td><td>Consumer</td><td>United States</td><td>Lewiston</td><td>Idaho</td><td>83501</td><td>West</td><td>OFF-BI-10004817</td><td>Office Supplies</td><td>Binders</td><td>GBC Personal VeloBind Strips</td><td>9.584</td><td>1</td><td>0.2</td><td>3.3544</td><td>2014</td><td>1</td></tr><tr><td>CA-2014-169460</td><td>2014-01-19</td><td>2014-01-21</td><td>Second Class</td><td>NF-18595</td><td>Nicole Fjeld</td><td>Home Office</td><td>United States</td><td>San Jose</td><td>California</td><td>95123</td><td>West</td><td>FUR-FU-10004017</td><td>Furniture</td><td>Furnishings</td><td>\"Executive Impressions 13\"\" Chairman Wall Clock\"</td><td>76.14</td><td>3</td><td>0.0</td><td>26.649</td><td>2014</td><td>1</td></tr><tr><td>CA-2014-110219</td><td>2014-01-01</td><td>2014-01-01</td><td>First Class</td><td>EB-13870</td><td>Emily Burns</td><td>Consumer</td><td>United States</td><td>San Antonio</td><td>Texas</td><td>78207</td><td>Central</td><td>FUR-CH-10001146</td><td>Furniture</td><td>Chairs</td><td>Global Value Mid-Back Manager's Chair, Gray</td><td>127.869</td><td>3</td><td>0.3</td><td>-9.13350000000001</td><td>2014</td><td>1</td></tr><tr><td>US-2014-165862</td><td>2014-01-13</td><td>2014-01-17</td><td>Standard Class</td><td>GK-14620</td><td>Grace Kelly</td><td>Corporate</td><td>United States</td><td>Los Angeles</td><td>California</td><td>90049</td><td>West</td><td>FUR-TA-10002855</td><td>Furniture</td><td>Tables</td><td>Bevis Round Conference Table Top & Single Column Base</td><td>351.216</td><td>3</td><td>0.2</td><td>4.39019999999998</td><td>2014</td><td>1</td></tr><tr><td>CA-2014-109491</td><td>2014-01-20</td><td>2014-01-26</td><td>Standard Class</td><td>LC-16930</td><td>Linda Cazamias</td><td>Corporate</td><td>United States</td><td>Richmond</td><td>Indiana</td><td>47374</td><td>Central</td><td>FUR-FU-10000221</td><td>Furniture</td><td>Furnishings</td><td>Master Caster Door Stop, Brown</td><td>20.32</td><td>4</td><td>0.0</td><td>6.9088</td><td>2014</td><td>1</td></tr><tr><td>CA-2014-109491</td><td>2014-01-20</td><td>2014-01-26</td><td>Standard Class</td><td>LC-16930</td><td>Linda Cazamias</td><td>Corporate</td><td>United States</td><td>Richmond</td><td>Indiana</td><td>47374</td><td>Central</td><td>TEC-AC-10001284</td><td>Technology</td><td>Accessories</td><td>Enermax Briskie RF Wireless Keyboard and Mouse Combo</td><td>62.31</td><td>3</td><td>0.0</td><td>22.4316</td><td>2014</td><td>1</td></tr><tr><td>CA-2014-116757</td><td>2014-01-30</td><td>2014-01-01</td><td>Standard Class</td><td>MS-17980</td><td>Michael Stewart</td><td>Corporate</td><td>United States</td><td>Houston</td><td>Texas</td><td>77095</td><td>Central</td><td>OFF-FA-10002815</td><td>Office Supplies</td><td>Fasteners</td><td>Staples</td><td>21.312</td><td>6</td><td>0.2</td><td>7.1928</td><td>2014</td><td>1</td></tr><tr><td>CA-2014-116757</td><td>2014-01-30</td><td>2014-01-01</td><td>Standard Class</td><td>MS-17980</td><td>Michael Stewart</td><td>Corporate</td><td>United States</td><td>Houston</td><td>Texas</td><td>77095</td><td>Central</td><td>OFF-PA-10002005</td><td>Office Supplies</td><td>Paper</td><td>Xerox 225</td><td>25.92</td><td>5</td><td>0.2</td><td>9.072</td><td>2014</td><td>1</td></tr><tr><td>CA-2014-147235</td><td>2014-01-24</td><td>2014-01-28</td><td>Standard Class</td><td>CD-11920</td><td>Carlos Daly</td><td>Consumer</td><td>United States</td><td>New York City</td><td>New York</td><td>10024</td><td>East</td><td>OFF-PA-10004948</td><td>Office Supplies</td><td>Paper</td><td>Xerox 190</td><td>24.9</td><td>5</td><td>0.0</td><td>11.703</td><td>2014</td><td>1</td></tr><tr><td>CA-2014-117016</td><td>2014-01-01</td><td>2014-01-01</td><td>Standard Class</td><td>SC-20095</td><td>Sanjit Chand</td><td>Consumer</td><td>United States</td><td>Margate</td><td>Florida</td><td>33063</td><td>South</td><td>OFF-AR-10001374</td><td>Office Supplies</td><td>Art</td><td>BIC Brite Liner Highlighters, Chisel Tip</td><td>15.552</td><td>3</td><td>0.2</td><td>2.3328</td><td>2014</td><td>1</td></tr><tr><td>CA-2014-130624</td><td>2014-01-21</td><td>2014-01-24</td><td>First Class</td><td>TB-21280</td><td>Toby Braunhardt</td><td>Consumer</td><td>United States</td><td>New York City</td><td>New York</td><td>10024</td><td>East</td><td>OFF-AP-10001303</td><td>Office Supplies</td><td>Appliances</td><td>Holmes Cool Mist Humidifier for the Whole House with 8-Gallon Output per Day, Extended Life Filter</td><td>59.7</td><td>3</td><td>0.0</td><td>26.865</td><td>2014</td><td>1</td></tr><tr><td>CA-2014-130624</td><td>2014-01-21</td><td>2014-01-24</td><td>First Class</td><td>TB-21280</td><td>Toby Braunhardt</td><td>Consumer</td><td>United States</td><td>New York City</td><td>New York</td><td>10024</td><td>East</td><td>TEC-PH-10003963</td><td>Technology</td><td>Phones</td><td>GE 2-Jack Phone Line Splitter</td><td>617.97</td><td>3</td><td>0.0</td><td>160.6722</td><td>2014</td><td>1</td></tr><tr><td>CA-2014-130624</td><td>2014-01-21</td><td>2014-01-24</td><td>First Class</td><td>TB-21280</td><td>Toby Braunhardt</td><td>Consumer</td><td>United States</td><td>New York City</td><td>New York</td><td>10024</td><td>East</td><td>OFF-PA-10003883</td><td>Office Supplies</td><td>Paper</td><td>\"Message Book, Phone, Wirebound Standard Line Memo, 2 3/4\"\" X 5\"\"\"</td><td>19.65</td><td>3</td><td>0.0</td><td>9.039</td><td>2014</td><td>1</td></tr><tr><td>US-2014-154879</td><td>2014-01-01</td><td>2014-01-11</td><td>Standard Class</td><td>SN-20710</td><td>Steve Nguyen</td><td>Home Office</td><td>United States</td><td>Los Angeles</td><td>California</td><td>90004</td><td>West</td><td>OFF-AR-10001897</td><td>Office Supplies</td><td>Art</td><td>Model L Table or Wall-Mount Pencil Sharpener</td><td>107.94</td><td>6</td><td>0.0</td><td>30.2232</td><td>2014</td><td>1</td></tr><tr><td>US-2014-154879</td><td>2014-01-01</td><td>2014-01-11</td><td>Standard Class</td><td>SN-20710</td><td>Steve Nguyen</td><td>Home Office</td><td>United States</td><td>Los Angeles</td><td>California</td><td>90004</td><td>West</td><td>OFF-LA-10004425</td><td>Office Supplies</td><td>Labels</td><td>Staple-on labels</td><td>5.78</td><td>2</td><td>0.0</td><td>2.7166</td><td>2014</td><td>1</td></tr><tr><td>CA-2014-127936</td><td>2014-01-01</td><td>2014-01-01</td><td>First Class</td><td>CL-12565</td><td>Clay Ludtke</td><td>Consumer</td><td>United States</td><td>New York City</td><td>New York</td><td>10009</td><td>East</td><td>OFF-AR-10002445</td><td>Office Supplies</td><td>Art</td><td>SANFORD Major Accent Highlighters</td><td>21.24</td><td>3</td><td>0.0</td><td>8.0712</td><td>2014</td><td>1</td></tr><tr><td>US-2014-138758</td><td>2014-01-01</td><td>2014-01-11</td><td>Standard Class</td><td>JL-15835</td><td>John Lee</td><td>Consumer</td><td>United States</td><td>Philadelphia</td><td>Pennsylvania</td><td>19120</td><td>East</td><td>FUR-FU-10003039</td><td>Furniture</td><td>Furnishings</td><td>\"Howard Miller 11-1/2\"\" Diameter Grantwood Wall Clock\"</td><td>69.008</td><td>2</td><td>0.2</td><td>12.0764</td><td>2014</td><td>1</td></tr><tr><td>US-2014-138758</td><td>2014-01-01</td><td>2014-01-11</td><td>Standard Class</td><td>JL-15835</td><td>John Lee</td><td>Consumer</td><td>United States</td><td>Philadelphia</td><td>Pennsylvania</td><td>19120</td><td>East</td><td>FUR-CH-10002880</td><td>Furniture</td><td>Chairs</td><td>Global High-Back Leather Tilter, Burgundy</td><td>172.186</td><td>2</td><td>0.3</td><td>-46.7362</td><td>2014</td><td>1</td></tr><tr><td>CA-2014-142965</td><td>2014-01-20</td><td>2014-01-20</td><td>Same Day</td><td>SW-20245</td><td>Scot Wooten</td><td>Consumer</td><td>United States</td><td>Springfield</td><td>Ohio</td><td>45503</td><td>East</td><td>OFF-BI-10000977</td><td>Office Supplies</td><td>Binders</td><td>Ibico Plastic Spiral Binding Combs</td><td>27.36</td><td>3</td><td>0.7</td><td>-21.888</td><td>2014</td><td>1</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "US-2014-150126",
         "2014-01-27",
         "2014-01-01",
         "Standard Class",
         "AS-10045",
         "Aaron Smayling",
         "Corporate",
         "United States",
         "New York City",
         "New York",
         10035,
         "East",
         "OFF-PA-10002709",
         "Office Supplies",
         "Paper",
         "Xerox 1956",
         65.78,
         11,
         0,
         32.2322,
         2014,
         1
        ],
        [
         "US-2014-121734",
         "2014-01-11",
         "2014-01-16",
         "Standard Class",
         "SE-20110",
         "Sanjit Engle",
         "Consumer",
         "United States",
         "Lewiston",
         "Idaho",
         83501,
         "West",
         "OFF-BI-10004817",
         "Office Supplies",
         "Binders",
         "GBC Personal VeloBind Strips",
         9.584,
         1,
         0.2,
         3.3544,
         2014,
         1
        ],
        [
         "CA-2014-169460",
         "2014-01-19",
         "2014-01-21",
         "Second Class",
         "NF-18595",
         "Nicole Fjeld",
         "Home Office",
         "United States",
         "San Jose",
         "California",
         95123,
         "West",
         "FUR-FU-10004017",
         "Furniture",
         "Furnishings",
         "\"Executive Impressions 13\"\" Chairman Wall Clock\"",
         76.14,
         3,
         0,
         26.649,
         2014,
         1
        ],
        [
         "CA-2014-110219",
         "2014-01-01",
         "2014-01-01",
         "First Class",
         "EB-13870",
         "Emily Burns",
         "Consumer",
         "United States",
         "San Antonio",
         "Texas",
         78207,
         "Central",
         "FUR-CH-10001146",
         "Furniture",
         "Chairs",
         "Global Value Mid-Back Manager's Chair, Gray",
         127.869,
         3,
         0.3,
         -9.13350000000001,
         2014,
         1
        ],
        [
         "US-2014-165862",
         "2014-01-13",
         "2014-01-17",
         "Standard Class",
         "GK-14620",
         "Grace Kelly",
         "Corporate",
         "United States",
         "Los Angeles",
         "California",
         90049,
         "West",
         "FUR-TA-10002855",
         "Furniture",
         "Tables",
         "Bevis Round Conference Table Top & Single Column Base",
         351.216,
         3,
         0.2,
         4.39019999999998,
         2014,
         1
        ],
        [
         "CA-2014-109491",
         "2014-01-20",
         "2014-01-26",
         "Standard Class",
         "LC-16930",
         "Linda Cazamias",
         "Corporate",
         "United States",
         "Richmond",
         "Indiana",
         47374,
         "Central",
         "FUR-FU-10000221",
         "Furniture",
         "Furnishings",
         "Master Caster Door Stop, Brown",
         20.32,
         4,
         0,
         6.9088,
         2014,
         1
        ],
        [
         "CA-2014-109491",
         "2014-01-20",
         "2014-01-26",
         "Standard Class",
         "LC-16930",
         "Linda Cazamias",
         "Corporate",
         "United States",
         "Richmond",
         "Indiana",
         47374,
         "Central",
         "TEC-AC-10001284",
         "Technology",
         "Accessories",
         "Enermax Briskie RF Wireless Keyboard and Mouse Combo",
         62.31,
         3,
         0,
         22.4316,
         2014,
         1
        ],
        [
         "CA-2014-116757",
         "2014-01-30",
         "2014-01-01",
         "Standard Class",
         "MS-17980",
         "Michael Stewart",
         "Corporate",
         "United States",
         "Houston",
         "Texas",
         77095,
         "Central",
         "OFF-FA-10002815",
         "Office Supplies",
         "Fasteners",
         "Staples",
         21.312,
         6,
         0.2,
         7.1928,
         2014,
         1
        ],
        [
         "CA-2014-116757",
         "2014-01-30",
         "2014-01-01",
         "Standard Class",
         "MS-17980",
         "Michael Stewart",
         "Corporate",
         "United States",
         "Houston",
         "Texas",
         77095,
         "Central",
         "OFF-PA-10002005",
         "Office Supplies",
         "Paper",
         "Xerox 225",
         25.92,
         5,
         0.2,
         9.072,
         2014,
         1
        ],
        [
         "CA-2014-147235",
         "2014-01-24",
         "2014-01-28",
         "Standard Class",
         "CD-11920",
         "Carlos Daly",
         "Consumer",
         "United States",
         "New York City",
         "New York",
         10024,
         "East",
         "OFF-PA-10004948",
         "Office Supplies",
         "Paper",
         "Xerox 190",
         24.9,
         5,
         0,
         11.703,
         2014,
         1
        ],
        [
         "CA-2014-117016",
         "2014-01-01",
         "2014-01-01",
         "Standard Class",
         "SC-20095",
         "Sanjit Chand",
         "Consumer",
         "United States",
         "Margate",
         "Florida",
         33063,
         "South",
         "OFF-AR-10001374",
         "Office Supplies",
         "Art",
         "BIC Brite Liner Highlighters, Chisel Tip",
         15.552,
         3,
         0.2,
         2.3328,
         2014,
         1
        ],
        [
         "CA-2014-130624",
         "2014-01-21",
         "2014-01-24",
         "First Class",
         "TB-21280",
         "Toby Braunhardt",
         "Consumer",
         "United States",
         "New York City",
         "New York",
         10024,
         "East",
         "OFF-AP-10001303",
         "Office Supplies",
         "Appliances",
         "Holmes Cool Mist Humidifier for the Whole House with 8-Gallon Output per Day, Extended Life Filter",
         59.7,
         3,
         0,
         26.865,
         2014,
         1
        ],
        [
         "CA-2014-130624",
         "2014-01-21",
         "2014-01-24",
         "First Class",
         "TB-21280",
         "Toby Braunhardt",
         "Consumer",
         "United States",
         "New York City",
         "New York",
         10024,
         "East",
         "TEC-PH-10003963",
         "Technology",
         "Phones",
         "GE 2-Jack Phone Line Splitter",
         617.97,
         3,
         0,
         160.6722,
         2014,
         1
        ],
        [
         "CA-2014-130624",
         "2014-01-21",
         "2014-01-24",
         "First Class",
         "TB-21280",
         "Toby Braunhardt",
         "Consumer",
         "United States",
         "New York City",
         "New York",
         10024,
         "East",
         "OFF-PA-10003883",
         "Office Supplies",
         "Paper",
         "\"Message Book, Phone, Wirebound Standard Line Memo, 2 3/4\"\" X 5\"\"\"",
         19.65,
         3,
         0,
         9.039,
         2014,
         1
        ],
        [
         "US-2014-154879",
         "2014-01-01",
         "2014-01-11",
         "Standard Class",
         "SN-20710",
         "Steve Nguyen",
         "Home Office",
         "United States",
         "Los Angeles",
         "California",
         90004,
         "West",
         "OFF-AR-10001897",
         "Office Supplies",
         "Art",
         "Model L Table or Wall-Mount Pencil Sharpener",
         107.94,
         6,
         0,
         30.2232,
         2014,
         1
        ],
        [
         "US-2014-154879",
         "2014-01-01",
         "2014-01-11",
         "Standard Class",
         "SN-20710",
         "Steve Nguyen",
         "Home Office",
         "United States",
         "Los Angeles",
         "California",
         90004,
         "West",
         "OFF-LA-10004425",
         "Office Supplies",
         "Labels",
         "Staple-on labels",
         5.78,
         2,
         0,
         2.7166,
         2014,
         1
        ],
        [
         "CA-2014-127936",
         "2014-01-01",
         "2014-01-01",
         "First Class",
         "CL-12565",
         "Clay Ludtke",
         "Consumer",
         "United States",
         "New York City",
         "New York",
         10009,
         "East",
         "OFF-AR-10002445",
         "Office Supplies",
         "Art",
         "SANFORD Major Accent Highlighters",
         21.24,
         3,
         0,
         8.0712,
         2014,
         1
        ],
        [
         "US-2014-138758",
         "2014-01-01",
         "2014-01-11",
         "Standard Class",
         "JL-15835",
         "John Lee",
         "Consumer",
         "United States",
         "Philadelphia",
         "Pennsylvania",
         19120,
         "East",
         "FUR-FU-10003039",
         "Furniture",
         "Furnishings",
         "\"Howard Miller 11-1/2\"\" Diameter Grantwood Wall Clock\"",
         69.008,
         2,
         0.2,
         12.0764,
         2014,
         1
        ],
        [
         "US-2014-138758",
         "2014-01-01",
         "2014-01-11",
         "Standard Class",
         "JL-15835",
         "John Lee",
         "Consumer",
         "United States",
         "Philadelphia",
         "Pennsylvania",
         19120,
         "East",
         "FUR-CH-10002880",
         "Furniture",
         "Chairs",
         "Global High-Back Leather Tilter, Burgundy",
         172.186,
         2,
         0.3,
         -46.7362,
         2014,
         1
        ],
        [
         "CA-2014-142965",
         "2014-01-20",
         "2014-01-20",
         "Same Day",
         "SW-20245",
         "Scot Wooten",
         "Consumer",
         "United States",
         "Springfield",
         "Ohio",
         45503,
         "East",
         "OFF-BI-10000977",
         "Office Supplies",
         "Binders",
         "Ibico Plastic Spiral Binding Combs",
         27.36,
         3,
         0.7,
         -21.888,
         2014,
         1
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "order_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "order_date",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "ship_date",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "ship_mode",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "customer_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "customer_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "segment",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "country",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "city",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "state",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "postal_code",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "region",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "product_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "category",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "sub_category",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "product_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "sales",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "quantity",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "discount",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "profit",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "year",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "month",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# try our writing partition\n",
    "\n",
    "try_df = spark.read.parquet('/Volumes/rawfile_csv/sss/silver/')\n",
    "try_df.select('*').where((col('year')==2014) & (col('month')==1)).limit(20).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3392c90d-b3b9-468f-91f0-e4c7001518a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Before we leave\n",
    "This is the end of bronze-to-silver notebook. I think it will be better if we use notebook for bronze to silver and silver to gold separately. See you in the next notebook (sss_silver_gold.ipynb)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "sss_bronze_silver",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
